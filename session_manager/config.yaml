# Session Manager Configuration
# VCA 1.0 - Phase 2

# ============================================================================
# LATENCY MONITORING CONFIGURATION (Easy-to-change parameters at top)
# ============================================================================
latency_monitoring:
  enabled: true
  target_total_latency: 10.0  # Target in seconds for total pipeline
  log_breakdown: true          # Log detailed breakdown after each request
  send_to_client: true         # Send metrics to Android app

  # Component-specific targets (in seconds)
  component_targets:
    vad: 0.1
    silence_detection: 1.5
    stt: 4.0
    llm: 3.0
    tts: 3.0
    websocket: 0.5

  # Model-specific expected latencies (for decision making)
  model_latencies:
    gpt-5:
      min: 3.0
      avg: 4.5
      max: 6.0
    gpt-5-mini:
      min: 2.0
      avg: 2.5
      max: 3.5
    gpt-5-nano:
      min: 0.5
      avg: 1.0
      max: 1.5
    gpt-4o:
      min: 2.5
      avg: 3.5
      max: 4.5

  # Automatic optimization suggestions
  optimization_suggestions: true

# ============================================================================
# CONVERSATION HISTORY CONFIGURATION (Easy-to-change parameters)
# ============================================================================
conversation:
  max_history_turns: 10  # Default number of conversation turns to keep
  max_history_turns_dev: 20  # Extended history for development/debugging
  trim_strategy: "sliding_window"  # or "summary" (future)

  # Context-aware settings (based on query type)
  context_aware: true
  tech_support_context: 15  # More context for tech support queries
  casual_context: 5  # Less for casual conversation

# ============================================================================
# LLM CONFIGURATION (Easy-to-change model selection)
# ============================================================================
llm:
  enabled: false  # Toggle: true=LLM mode, false=echo mode (for testing)

  # Model selection - easily changeable for testing different variants
  model_variants:
    development: "gpt-5"        # Complex reasoning for tech support/debugging
    production: "gpt-5-mini"    # Balanced speed/cost for daily use
    low_latency: "gpt-5-nano"   # Ultra-fast responses for simple queries

  current_model: "gpt-5-mini"  # Default model to use

  # GPT-5 specific settings (per OpenAI docs)
  reasoning_effort: "medium"    # low/medium/high (low for faster responses)
  text_verbosity: "low"         # low/medium/high (low for concise responses)

  # Response guidelines
  max_response_sentences: 3  # Target response length (can be overridden for tech support)

  # System prompt for Warren
  system_prompt: |
    You are Warren's voice assistant named Nabu.
    Address the user as "Warren" when appropriate.

    Response Guidelines:
    - DEFAULT: Keep responses to 1-3 sentences for conversational queries
    - TECH SUPPORT: Provide step-by-step instructions (can be longer)
    - When Warren says "explain" or "tell me more", provide detailed explanation

    Main Functions:
    1. TECH SUPPORT (primary during development phase)
    2. REMINDERS & NOTES
    3. COMMUNICATION ASSISTANCE
    4. GENERAL CONVERSATION

    Be patient, clear, and friendly. Warren may have slurred speech, so be understanding.

# Server Configuration
server:
  host: "0.0.0.0"
  port: 5000
  debug: true

# OpenAI Configuration
openai:
  # API key loaded from environment variable OPENAI_API_KEY

  # Speech-to-Text (Whisper)
  stt:
    model: "whisper-1"
    language: "en"
    response_format: "text"
    temperature: 0.0  # 0.0 for more deterministic transcription

  # Text-to-Speech
  tts:
    model: "tts-1"
    voice: "nova"  # Options: alloy, echo, fable, onyx, nova, shimmer
    speed: 1.0

# Home Assistant Integration
homeassistant:
  url: "http://localhost:8123"
  websocket_url: "ws://localhost:8123/api/websocket"
  # Access token loaded from environment variable HA_ACCESS_TOKEN

# Session Management
session:
  # Voice Activity Detection
  vad:
    enabled: true
    silence_timeout: 2.0  # seconds of silence before processing transcript
    min_speech_duration: 0.5  # minimum speech duration to process (seconds)
    sample_rate: 16000  # audio sample rate (Hz)
    frame_duration: 30  # VAD frame duration in ms (10, 20, or 30)
    aggressiveness: 3  # VAD aggressiveness (0-3, higher = more aggressive)

  # Stop phrases that end the session
  stop_phrases:
    - "that's all"
    - "stop listening"
    - "thank you goodbye"
    - "goodbye"

  # Timeouts
  max_session_duration: 300  # 5 minutes max per session
  wake_word_cooldown: 1.0  # seconds between wake-word detections

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/session_manager.log"
  console: true  # Also log to console

# ============================================================================
# PROVIDER SELECTION (Easy switching for experimentation)
# ============================================================================
# STT Provider Selection
stt_provider: "pytorch_whisper"  # Options: openai_whisper, mock_stt, local_whisper, pytorch_whisper

# TTS Provider Selection
tts_provider: "openai_tts"  # Options: openai_tts, mock_tts, coqui_tts, piper_tts

# Mock Provider Configuration (for testing without API calls)
mock_stt:
  mock_latency: 0.5  # Fast mock STT (0.5 seconds)
  mock_text: "Hello, this is a test transcription"
  mock_confidence: 0.98

mock_tts:
  mock_latency: 0.3  # Fast mock TTS (0.3 seconds)
  audio_format: "mp3"
  sample_rate: 24000

# Local Whisper Configuration (faster-whisper with CT Translate2)
# NOTE: GTX 970 Maxwell not fully supported by ctranslate2 for GPU acceleration
local_whisper:
  model_size: "small"  # Options: tiny, base, small, medium, large
  device: "cpu"        # cpu mode (GTX 970 Maxwell limited GPU support)
  compute_type: "int8"  # int8 works well on CPU
  language: "en"       # Target language for transcription
  beam_size: 5         # Beam search size (higher = more accurate but slower)
  vad_filter: true     # Use Voice Activity Detection filter

# PyTorch Whisper Configuration (OpenAI Whisper with PyTorch - RECOMMENDED for GTX 970)
# Fully supports GTX 970 Maxwell using FP32 mode
pytorch_whisper:
  model_size: "small"  # Options: tiny, base, small, medium, large
                       # small recommended for GTX 970 (2GB VRAM, 3-4s latency)
  device: "cuda"       # cuda (GTX 970) or cpu
  fp16: false          # CRITICAL: Must be false for Maxwell (use FP32 mode)
                       # Maxwell (GTX 970) is 64x slower at FP16, use FP32 instead
  language: "en"       # Target language for transcription
  temperature: 0.0     # 0.0 = deterministic, 0.2-0.4 = more creative/flexible
  beam_size: 5         # Beam search size (1-10, higher = more accurate but slower)
  initial_prompt: null  # Optional: "This speech may be slurred or unclear"
  condition_on_previous_text: true  # Use context from previous utterances

# Coqui TTS Configuration (XTTS-v2 - Session 11)
# Local neural TTS with PyTorch CUDA acceleration for GTX 970 Maxwell
# Uses same PyTorch 2.2.2+cu121 stack as Whisper STT (DO NOT UPGRADE)
coqui_tts:
  model_name: "tts_models/multilingual/multi-dataset/xtts_v2"  # XTTS-v2 model (~2GB)
  use_gpu: true         # Leverage existing PyTorch CUDA stack (GTX 970 FP32 mode)
  language: "en"        # Target language for synthesis
  speed: 1.0            # Speech speed (1.0 = normal, adjust for Warren's preference)
  sample_rate: 16000    # VCA expects 16kHz (XTTS native is 24kHz, auto-resampled)
  reference_audio: null # Optional: Path to audio file for voice cloning
                        # Example: "data/reference_voices/dad_voice.wav"
                        # Leave null for default XTTS voices

# Piper TTS Configuration (Session 11 - RECOMMENDED)
# Fast, lightweight local TTS using ONNX Runtime (CPU-optimized)
# Expected latency: 0.2-0.5s (vs 3.0s OpenAI, 9.0s XTTS-v2)
piper_tts:
  model_path: "models/piper/en_US-lessac-medium.onnx"  # 61MB model
  config_path: "models/piper/en_US-lessac-medium.onnx.json"  # Auto-detected if not specified
  speaker_id: null      # Optional: For multi-speaker models
  length_scale: 1.0     # Speed control (1.0 = normal, <1.0 = faster, >1.0 = slower)
  noise_scale: 0.667    # Variability in synthesis (0.0-1.0)
  noise_w: 0.8          # Phoneme duration variability (0.0-1.0)
  sample_rate: 16000    # VCA expects 16kHz (Piper native is 22050Hz, auto-resampled)
